--- 
title: "HR Analytics in R"
author: "Hendrik Feddersen"
date: "`r format(Sys.time(), '%B %d, %Y')`"
knit: "bookdown::render_book"
biblio-style: apalike
bibliography:
- bib/books.bib
- bib/packages.bib
- bib/articles.bib
colorlinks: yes
apple-touch-icon: images/logos/favicons/apple-touch-icon.png
description: "An open-source self-training book to assist you in your journey to sustainable HR Analytics"
documentclass: krantz
favicon: images/logos/favicons/favicon.ico
fontsize: 12pt, krantz2
github-repo: Hendrik147/HR_Analytics_in_R_book
graphics: yes
link-citations: yes
lof: yes
lot: yes
monofont: "Source Code Pro"
monofontoptions: "Scale=0.7"
site: bookdown::bookdown_site
subtitle: Common tasks achieved with the power of R
always_allow_html: yes
url: 'https\://hranalytics.netlify.com/'
cover-image: images/logos/book_cover.png
---

# Introduction {#intro}

Placeholder


## Introduction for students {#sec:intro-for-students}
### Working with the material {#workingwithmaterials .unnumbered}
### Conventions {#conventions .unnumbered}
### What you will learn from this book {#subsec:learning-goals}
### Data/science pipeline {#subsec:pipeline}
### Reproducible research {#subsec:reproducible}
### Final note for students
## Introduction for instructors {#sec:intro-instructors}
### Who is this book for?
## Connect and contribute {#sec:connect-contribute}
## About this book {#sec:about-book}
## About the author {#sec:about-authors}
### Colophon 

<!--chapter:end:index.Rmd-->


# Getting Started with Data in R {#getting-started}

Placeholder


## What are R and RStudio? {#r-rstudio}
### Installing R and RStudio
### Using R via RStudio
## How do I code in R? {#code}
### Basic programming concepts and terminology {#programming-concepts}
### Errors, warnings, and messages
### Tips on learning to code
## What are R packages? {#packages}
### Package installation {#package-installation}
### Package loading {#package-loading}
### Package use {#package-use}
## Explore your first datasets {#nycflights13}
### `nycflights13` package
### `flights` data frame
### Exploring data frames {#exploredataframes}
### Identification & measurement variables {#identification-vs-measurement-variables}
### Help files
## Conclusion
### Additional resources
### What's to come?

<!--chapter:end:02-getting-started.Rmd-->


# (PART) Data Science via the tidyverse {-}
# Data Visualization {#viz}

Placeholder


### Needed packages {-}
## The Grammar of Graphics {#grammarofgraphics}
### Components of the Grammar
### Gapminder data {#gapminder}
### Other components
### ggplot2 package
## Five Named Graphs - The 5NG {#FiveNG}
## 5NG#1: Scatterplots {#scatterplots}
### Scatterplots via geom_point {#geompoint}
### Over-plotting {#overplotting}
### Summary
## 5NG#2: Linegraphs {#linegraphs}
### Linegraphs via geom_line {#geomline}
### Summary
## 5NG#3: Histograms {#histograms}
### Histograms via geom_histogram {#geomhistogram}
### Adjusting the bins {#adjustbins}
### Summary
## Facets {#facets}
## 5NG#4: Boxplots {#boxplots}
### Boxplots via geom_boxplot {#geomboxplot}
### Summary
## 5NG#5: Barplots {#geombar}
### Barplots via geom_bar or geom_col
### Must avoid pie charts!
### Two categorical variables {#two-categ-barplot}
### Summary
## Conclusion
### Summary table
### Argument specification
### Additional resources
### What's to come {#whats-to-come-3}

<!--chapter:end:03-visualization.Rmd-->


# Data Wrangling {#wrangling}

Placeholder


### Needed packages {-}
## The pipe operator: `%>%` {#piping}
## `filter` rows {#filter}
## `summarize` variables {#summarize}
## `group_by` rows {#groupby}
### Grouping by more than one variable
## `mutate` existing variables {#mutate}
## `arrange` and sort rows {#arrange}
## `join` data frames {#joins}
### Matching "key" variable names
### Different "key" variable names {#diff-key}
### Multiple "key" variables
### Normal forms
## Other verbs {#other-verbs}
### `select` variables {#select}
### `rename` variables {#rename}
### `top_n` values of a variable
## Conclusion
### Summary table
### Additional resources
### What's to come?

<!--chapter:end:04-wrangling.Rmd-->


# Data Importing & "Tidy" Data {#tidy}

Placeholder


### Needed packages {-}
## Importing data {#csv}
### Using the console
### Using RStudio's interface
## Tidy data {#tidy-data-ex}
### Definition of "tidy" data
### Converting to "tidy" data
### `nycflights13` package
## Case study: Democracy in Guatemala {#case-study-tidy}
## Conclusion
### `tidyverse` package {#tidyverse-package}
### Additional resources
### What's to come?

<!--chapter:end:05-tidy.Rmd-->


# (PART) Data Modeling via moderndive {-}
# Basic Regression {#regression}

Placeholder


### Needed packages {-}
## One numerical explanatory variable {#model1}
### Exploratory data analysis {#model1EDA}
### Simple linear regression {#model1table}
### Observed/fitted values and residuals {#model1points}
## One categorical explanatory variable {#model2}
### Exploratory data analysis {#model2EDA}
### Linear regression {#model2table}
### Observed/fitted values and residuals {#model2points}
## Related topics
### Correlation is not necessarily causation {#correlation-is-not-causation}
### Best fitting line {#leastsquares}
### `get_regression_x()` functions {#underthehood}
## Conclusion
### Additional resources {#additional-resources-basic-regression}
### What's to come?

<!--chapter:end:06-regression.Rmd-->


# Multiple Regression {#multiple-regression}

Placeholder


### Needed packages {-}
## One numerical & one categorical explanatory variable {#model4}
### Exploratory data analysis {#model4EDA}
### Interaction model {#model4interactiontable}
### Parallel slopes model {#model4table}
### Observed/fitted values and residuals {#model4points}
## Two numerical explanatory variables {#model3}
### Exploratory data analysis {#model3EDA}
### Regression plane {#model3table}
### Observed/fitted values and residuals {#model3points}
## Related topics
### Model selection {#model-selection}
### Correlation coefficient {#correlationcoefficient2}
### Simpson's Paradox {#simpsonsparadox}
## Conclusion
### Additional resources
### What's to come?

<!--chapter:end:07-multiple-regression.Rmd-->


# (PART) Statistical inference via infer {-}
# Sampling {#sampling}

Placeholder


### Needed packages {-}
## Sampling activity {#sampling-activity}
### What proportion of this bowl's balls are red?
### Using the shovel once 
### Using the shovel 33 times {#student-shovels}
### What are we doing here?
## Computer simulation of sampling {#sampling-simulation}
### Using the virtual shovel once
### Using the virtual shovel 33 times
### Using the virtual shovel 1000 times {#shovel-1000-times}
### Using different shovels {#different-shovels}
## Sampling framework {#sampling-framework}
### Terminology & notation {#terminology-and-notation}
### Statistical definitions
### The moral of the story
## Case study: Polls {#sampling-case-study}
## Conclusion {#sampling-conclusion}
### Random sampling vs random assignment {#sampling-conclusion-sampling-vs-assignment}
### Central Limit Theorem {#sampling-conclusion-central-limit-theorem}
### Summary table {#sampling-conclusion-table}
### Additional resources
### What's to come?

<!--chapter:end:08-sampling.Rmd-->


# Confidence Intervals {#confidence-intervals}

Placeholder


### Needed packages {-}
## Resampling activity
### What is the average year of circulated US pennies in 2019?
#### Exploratory data analysis on original sample {-}
### Using resampling once
#### Exploratory data analysis on the resample {-}
### Using resampling 33 times {#student-resamples}
### What's the plan?
## Computer simulation of resampling {#resampling-simulation}
### Using the virtual resample once
### Using the virtual resample 33 times
### Using the virtual resample 1000 times
## Confidence interval build-up {#ci-build-up}
### The percentile method {#percentile-method}
### The standard error method
## The bootstrapping framework {#bootstrap-process}
### The original workflow needed for this
### The infer package for statistical inference
#### Specify variables {-}
#### Generate replicates {-}
#### Calculate summary statistics {-}
#### Observed statistic / point estimate calculations {-}
#### Visualize the results {-}
### Building confidence intervals with the infer package {#infer-ci}
### The percentile method with infer {#percentile-method-infer}
### The standard error method with infer
## Case study: Revisiting the red ball example {#one-prop-ci}
### Observed statistic
### Bootstrap distribution for one proportion {#one-prop-boot}
## Interpreting the confidence interval
#### Back to our pennies example {-}
### The width of confidence intervals {-}
#### The impact of confidence levels {-}
#### The impact of sample size {-}
## Case study: Comparing two proportions {#case-study-two-prop-ci}
### Compute the point estimate
### Bootstrap distribution
## Conclusion {#ci-conclusion}
### Comparing bootstrap and sampling distributions
#### Sampling distribution {-}
#### Bootstrap distribution {-}
### Theory-based confidence intervals {#theory-ci}
#### Procedure for building a theory-based CI for $p$ {-}
#### Confidence intervals based on 33 tactile samples {-}
#### Confidence intervals based on 100 virtual samples {-}
#### Where does the 1.96 come from? {-}
### Summary table {#ci-conclusion-table}
### Additional resources
### What's to come?

<!--chapter:end:09-confidence-intervals.Rmd-->


# Hypothesis Testing {#hypothesis-testing}  

Placeholder


### Needed packages {-}
## Hypothesis testing activity {#ht-activity}
### Question of interest
### What did we actually observe?
### Using permuting once
### Using permuting 33 times
## Hypothesis testing with infer {#ht-infer}
### Revisiting the infer verb framework
### The `infer` pipeline for the activity
#### Choose the variables of interest {-}
#### Set the model for the null hypothesis {-}
#### Replicate samples assuming the null hypothesis is true {-}
#### Compute the statistic for each replicate {-}
### The "There Is Only One Test" framework {#only-one-test}
## The p-value {#p-value}
### Corresponding confidence interval
### Summary
## Interpretation of hypothesis testing results {#ht-interpretation}
### Criminal trial analogy {#trial}
#### Two possible conclusions {-}
### Types of errors in hypothesis testing
#### Logic of hypothesis testing {-}
### Statistical significance
## Case study: comparing two means {#ht-case-study}
### Randomization/permutation
### Comparing action and romance movies
### Sampling $\rightarrow$ randomization
### Data
### Model of $H_0$
### Test statistic $\delta$  
### Observed effect $\delta^*$
### Simulated data
### Distribution of $\delta$ under $H_0$
### The p-value
### Corresponding confidence interval
## Conclusion
### When inference is not needed
### Problems with p-values
### Comparing confidence intervals and hypothesis tests
### Summary table {#ht-conclusion-table}
### Building theory-based methods using computation {#theory-hypo}
#### Example: $t$-test for two independent samples {-}
#### Conditions for t-test {-}
### Additional resources
### What's to come

<!--chapter:end:10-hypothesis-testing.Rmd-->


# Inference for Regression {#inference-for-regression}

Placeholder


### Needed packages {-}
## Simulation-based Inference for Regression
### Data
### Test statistic $\delta$
### Observed effect $\delta^*$
### Model of $H_0$
### Simulated data
### Distribution of $\delta$ under $H_0$
### The p-value
## Bootstrapping for the regression slope
## Inference for multiple regression
### Refresher: Professor evaluations data
### Refresher: Visualizations
### Refresher: Regression tables
### Script of R code
## Residual analysis
### Residual analysis {#model1residuals}
### Residual analysis {#model2residuals}
### Residual analysis {#model3residuals}
### Residual analysis {#model4residuals}

<!--chapter:end:11-inference-for-regression.Rmd-->


# (PART) Learnings so far {-}
# Thinking with Data {#thinking-with-data}

Placeholder


### Needed packages {-}
## Case study: Seattle house prices {#seattle-house-prices}
### Exploratory data analysis (EDA) {#house-prices-EDA-I}
### log10 transformations {#log10-transformations}
### EDA Part II
### Regression modeling {#house-prices-regression}
### Inference for regression {#house-prices-inference-for-regression}
### Making predictions {#house-prices-making-predictions}
## Case study: Effective data storytelling {#data-journalism}
### Bechdel test for Hollywood gender representation
### US Births in 1999
### Other examples
### Script of R code
## Concluding remarks {-}

<!--chapter:end:12-thinking-with-data.Rmd-->


# (PART) Practical examples on HR Analytics {-}
# Gender Pay Gap {#pay-gap}

Placeholder


## Data Cleaning and Prep.
## Summary Statistics by gender
## Avoiding Simpson's Paradox
## Model Estimation: OLS with controls. 
### Logarithm of Base Pay
### Results by Department
### Results by Job Title 

<!--chapter:end:13-pay_gap.Rmd-->


# Stop the formal performance management process {#stop-appraisals}

Placeholder



<!--chapter:end:14-stop_appraisals.Rmd-->


# HR Service Desk {#service_desk}

Placeholder



<!--chapter:end:15-service_desk.Rmd-->


# Personality insights {#personality}

Placeholder



<!--chapter:end:16-personality.Rmd-->


# Commuting time {#commuting_time}

Placeholder



<!--chapter:end:17-commuting_time.Rmd-->


# Module - Organisational network analysis {#orgnisational_network}

Placeholder



<!--chapter:end:18-organisational_network.Rmd-->


# Job classification analysis {#job_classification}

Placeholder


### Final evaluation of the various models

<!--chapter:end:19-job_classification.Rmd-->


# Masking HR data {#masking_data}

Placeholder


### Whitehouse dataset
### Fertility dataset

<!--chapter:end:20-masking_data.Rmd-->


# Absenteeism data {#absenteeism_data@MFG}

Placeholder


### For expediency we will delete the problem records in the dataset.

<!--chapter:end:21-absenteeism@MFG.Rmd-->


# Absenteeism data {#absenteeism_data@work}

Placeholder


## Data reading

<!--chapter:end:22-absenteeism@work.Rmd-->


# Accidents at work {#accidents_at_work}

Placeholder



<!--chapter:end:23-accidents_at_work.Rmd-->

# Attrition {#attrition}

```{r attrition, include=FALSE, purl=FALSE}
chap <- 24
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

options(scipen = 99, digits = 3)

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```


Here we introduce attrition.


<!--chapter:end:23-attrition.Rmd-->

# Interview attendance problem {#interview-attendance}

```{r interview-attendance, include=FALSE, purl=FALSE}
chap <- 24
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

options(scipen = 99, digits = 3)

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```


The dataset consists of details of nore than 1200 candidates and the interviews they have attended during the course of the period 2014-2016. 

The following are the variables columns:

- Date of Interview refers to the day the candidates were scheduled for the interview. The formats vary.
- Client that gave the recruitment vendor the requisite mandate
- Industry refers to the sector the client belongs to (Candidates can job hunt in vrious industries.)
- Location refers to the current location of the candidate.
- Position to be closed: Niche refers to rare skill sets, while routine refers to more common skill sets.
- Nature of Skillset refers to the skill the client has and specifies the same.
- Interview Type: There are three interview types: 
  1. Walk in drives - These are unscheduled. Candidates are either contacted or they come to the interview on their own volition, 
  2. Scheduled - Here the candidates profiles are screened by the client and subsequent to this, the vendor fixes an appointment between the client and the candidate. 
  3. The third one is a scheduled walkin. Here the number of candidates is larger and the candidates are informed beforehand of a tentative date to ascertain their availability. The profiles are screened as in a scheduled interview. In a sense it bears features of both a walk-in and a scheduled interview.
- Name( Cand ID) This is a substitute to keep the candidates identity a secret
- Gender
- Candidate Current Location
- Candidate Job Location
- Interview Venue
- Candidate Native location
- Have you obtained the necessary permission to start at the required time?
- I hope there will be no unscheduled meetings.
- Can I call you three hours before the interview and follow up on your attendance for the interview?
- Can I have an alternative telephone number? I assure you that I will not trouble you too much.
- Have you taken a printout of your updated resume? Have you read the JD and understood it?
- Are you clear with the venue details and the landmark?
- Has the call letter been shared
- Expected Attendance: Whether the candidate was expected to attend the interview. Here the alternatives are yes, no or uncertain.
- Observed Attendance: Whether the candidate attended the interview. This is binary and will form our dependent variable to be predicted.
- Marital Status: Single or married.

Source: https://www.kaggle.com/hugohk/learning-ml-with-caret

## Data reading

Ensure all needed libraries are installed

```{r include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(plyr)) install.packages("plyr")
```

```{r}
library(tidyverse)
```

```{r eval=FALSE}
interview_attendance <- read_csv("https://hranalyticslive.netlify.com/data/interview.csv")
```

```{r read_interview_data, echo=FALSE, warning=FALSE, message=FALSE}
interview_attendance <- read_csv("data/interview.csv")
```

```{r}
head(interview_attendance, 5)
```

# Data cleaning

```{r}

interview_attendance <- interview_attendance[-1234,] #remove the last row that contains only missing values

interview_attendance$X24   <- NULL #get rid of unnecesary columns on the right side
interview_attendance$X25 <- NULL #get rid of unnecesary columns on the right side
interview_attendance$X26 <- NULL #get rid of unnecesary columns on the right side
interview_attendance$X27 <- NULL #get rid of unnecesary columns on the right side
interview_attendance$X28 <- NULL #get rid of unnecesary columns on the right side

# Here we create a vector with column titles
mycolnames <-c('date_of_interview','client_name','industry','location','position',
               'skillset','interview_type','name', 'gender','current_location',
               'cjob_location','interview_venue','cnative_location',
               'permission_obtained','unscheduled_meetings','call_three_hours_before',
               'alternative_number','printout_resume_jd','clear_with_venue',
               'letter_been_shared','expected_attendance','observed_attendance',
               'marital_status')  

#Here we assign the previously defined column titles
colnames(interview_attendance) <- mycolnames 

rm(mycolnames) # Here we remove the mycolnames vector, as it is not required anymore

interview_attendance<- mutate_all(interview_attendance, funs(tolower)) # Sets all words to lower case

# Cancels all empty spaces in the observed attendance column
interview_attendance$observed_attendance <- gsub(" ", "", interview_attendance$observed_attendance)

# Cancels all empty spaces in the location column
interview_attendance$location <- gsub(" ", "", interview_attendance$location) 

# Cancels all empty spaces in the interview type column
interview_attendance$interview_type <- gsub(" ", "", interview_attendance$interview_type)

# Corrects a typo in the interview type column
interview_attendance$interview_type <- gsub("sceduledwalkin", "scheduledwalkin", interview_attendance$interview_type)

# Cancels all empty spaces in the candidate current location column
interview_attendance$current_location <- gsub(" ", "", interview_attendance$current_location)


#Converts values from character to numbers for Yes/no answers, just to keep things simple.
  colstoyesno <- c(14:22) # Here we define which column numbers to look at.
for (i in 1:length(colstoyesno)){   # Here we tell R to examine all variables in the previously defined columns
  j <- colstoyesno[i]
  interview_attendance[,j][interview_attendance[,j] !="yes"] <- "no"   
  interview_attendance[,j][is.na(interview_attendance[,j]) == TRUE] <- "no"
  #With the previous two lines all values different to yes, become a no, i.e. "uncertain" and "NA" are set to a "no".
}
rm(colstoyesno, i, j) #Here we remove the three just created vectors as a claen up.
```

In the following step we figure out what is the content of each relevant column and identify its unique values. Once we have done that a number is assigned for a later step.

```{r}
dir.create("codefiles_interview_attendance", showWarnings = FALSE)
#detach("package:plyr", unload = TRUE)
for(i in 1:length(colnames(interview_attendance))){
  vvar <- colnames(interview_attendance)[i]
  outdata <- interview_attendance %>% group_by(.dots = vvar) %>% count(.dots = vvar)
  outdata$idt <- LETTERS[seq(from=1, to=nrow(outdata))]
  outdata$id  <- row.names(outdata)
  outfile <- paste0("codefiles_interview_attendance/", vvar, ".csv")
  write.csv(outdata, file=outfile, row.names = FALSE)
}
rm(outdata, i, outfile, vvar)
```

This step uses the csv files created in the previous step and replaces the words with a number (except for the variable that we would like to predict) in order to prepare the data for machine learning.

```{r}
colstomap <- c(2:7, 9:21, 23)
library(plyr)

for(i in 1:length(colstomap)){
  j <- colstomap[i]
  vfilename <- paste0("codefiles_interview_attendance/", colnames(interview_attendance)[j], ".csv")
  dfcodes <- read_csv(vfilename)
  vfrom <- as.vector(dfcodes[,1])
  vto <- as.vector(dfcodes[,4])
  interview_attendance[,j] <- mapvalues(interview_attendance[,j], from=vfrom, to=vto) #Items in interview_attendance[,j] that match items from will be replaced by items in to, matched by position.
  interview_attendance[,j] <- as.integer(interview_attendance[,j])
}
rm(colstomap, i, j, vfilename, vfrom, vto, dfcodes)
```

Here we pick the data that are needed for the predictor.

```{r}
interview_attendanceml <- interview_attendance %>% select(-date_of_interview, -name)
interview_attendanceml <- interview_attendanceml %>% select(client_name:expected_attendance, 
                                observed_attendance)
head(interview_attendanceml, 5)
```

Here we start the real machine learning part. The training dataset is created containing 75% of the observations [interview_attendanceml_train] and the remaining observations are assigned to the test dataset [interview_attendanceml_test].

```{r}

library(caret) # Calls the caret library

set.seed(144) # Sets a seed for reproducability

index <- createDataPartition(interview_attendanceml$observed_attendance, p=0.75, list=FALSE) #Creates an index vector of the length of all the observations

interview_attendanceml_train <- interview_attendanceml[index,] # Creates a subset of 75% of data for training dataset
interview_attendanceml_test  <- interview_attendanceml[-index,] # Creates a subset of the remaining 25% of data for test dataset

rm(index,interview_attendanceml)
```

## Choosing a model

We didn't want to tune anything and just let the code figure it out. Since the output is 1 for a no show and 2 for a show, we decided to use the gbm method (generalized boosting machine) from caret for creating the algorithm.

# Training

We use the train function of the caret library to train the training data set determined in the previous step.

```{r}
myml_model <- train(interview_attendanceml_train[,1:19], interview_attendanceml_train[,20], method='gbm')

summary(myml_model)

predictions <- predict(object = myml_model, interview_attendanceml_test,
                       type = 'raw')

head(predictions)

print(postResample(pred=predictions, obs=as.factor(interview_attendanceml_test[,20])))
```

<!--chapter:end:24-interview_attendance.Rmd-->


# Ranking Medical Schools {#ranking-medical_school}

Placeholder



<!--chapter:end:25-ranking_medical_schools.Rmd-->


# Webscraping LinkedIn {#webscraping-linkedin}

Placeholder



<!--chapter:end:26-webscraping_linkedin.Rmd-->

# HR dashboards {#flexdashboards}

```{r flexdashboards, include=FALSE, purl=FALSE}
chap <- 27
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

options(scipen = 99, digits = 3)

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```


Here we introduce flexdashboards

<!--chapter:end:27-flexdashboards.Rmd-->

# HR Analytics product with Shiny {#shiny}

```{r shiny, include=FALSE, purl=FALSE}
chap <- 28
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

options(scipen = 99, digits = 3)

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```


Shiny is a very powerful framework for building web applications based on R. It is out of the scope of this book to make a comprehensive introduction to Shiny (which is too big a topic). We recommend that readers who are not familiar with Shiny learn more about it from the website https://shiny.rstudio.com before reading this chapter.

Unlike the more traditional workflow of creating static reports, you can create documents that allow your readers to change the parameters underlying your analysis and see the results immediately in Shiny R Markdown documents. In the example shown in Figure @ref(fig:shiny), the histogram will be automatically updated to reflect the number of bins selected by the reader.

A picture is worth a thousand words, and a Shiny document can potentially show you a thousand pictures as you interact with it. The readers are no longer tied to the fixed analysis and conclusions in the report. They may explore other possibilities by themselves, and possibly make new discoveries or draw different conclusions.

<!--chapter:end:28-data_science_product.Rmd-->


# (APPENDIX) Appendix {-}
# Statistical Background {#appendixA}

Placeholder


## Basic statistical terms
### Mean
### Median
### Standard deviation
### Five-number summary
### Distribution
### Outliers
## Normal distribution {#normal-curve}

<!--chapter:end:91-appendixA.Rmd-->


# Inference Examples {#appendixB}

Placeholder


## Needed packages {-}
## Inference mind map
## One mean
### Problem statement
### Competing hypotheses
#### In words {-}
#### In symbols (with annotations) {-}
#### Set $\alpha$ {-}
### Exploring the sample data
#### Guess about statistical significance {-}
### Non-traditional methods
#### Bootstrapping for hypothesis test {-}
##### Calculate $p$-value {-}
#### Bootstrapping for confidence interval {-}
### Traditional methods
#### Check conditions {-}
#### Test statistic {-}
##### Observed test statistic {-}
#### Compute $p$-value {-}
#### State conclusion {-}
#### Confidence interval {-}
### Comparing results
## One proportion
### Problem statement
### Competing hypotheses
#### In words {-}
#### In symbols (with annotations) {-}
#### Set $\alpha$ {-}
### Exploring the sample data
#### Guess about statistical significance {-}
### Non-traditional methods
#### Simulation for hypothesis test {-}
##### Calculate $p$-value {-}
#### Bootstrapping for confidence interval {-}
### Traditional methods
#### Check conditions {-}
#### Test statistic {-}
##### Observed test statistic {-}
#### Visualize and compute $p$-value {-}
#### State conclusion {-}
### Comparing results
## Two proportions
### Problem statement
### Competing hypotheses
#### In words {-}
#### Another way in words {-}
#### In symbols (with annotations) {-}
#### Set $\alpha$ {-}
### Exploring the sample data
#### Guess about statistical significance {-}
### Non-traditional methods
#### Collecting summary info {-}
#### Randomization for hypothesis test {-}
##### Calculate $p$-value {-}
#### Bootstrapping for confidence interval {-}
### Traditional methods
### Check conditions
### Test statistic
#### Observed test statistic {-}
### State conclusion
### Comparing results
## Two means (independent samples)
### Problem statement
### Competing hypotheses
#### In words {-}
#### Another way in words {-}
#### In symbols (with annotations) {-}
#### Set $\alpha$ {-}
### Exploring the sample data
#### Guess about statistical significance {-}
### Non-traditional methods
#### Collecting summary info {-}
#### Randomization for hypothesis test {-}
##### Calculate $p$-value {-}
#### Bootstrapping for confidence interval {-}
### Traditional methods
##### Check conditions {-}
### Test statistic
#### Observed test statistic {-}
### Compute $p$-value
### State conclusion
### Comparing results
## Two means (paired samples)
#### Problem statement {-}
### Competing hypotheses
#### In words {-}
#### In symbols (with annotations) {-}
#### Set $\alpha$ {-}
### Exploring the sample data
#### Guess about statistical significance {-}
### Non-traditional methods
#### Bootstrapping for hypothesis test {-}
##### Calculate $p$-value {-}
#### Bootstrapping for confidence interval {-}
### Traditional methods
#### Check conditions {-}
#### Test statistic {-}
##### Observed test statistic {-}
#### Compute $p$-value {-}
#### State conclusion {-}
### Comparing results

<!--chapter:end:92-appendixB.Rmd-->


# Reach for the Stars {#appendixC}

Placeholder


## Needed packages {-}
## Sorted barplots
## Interactive graphics
### Interactive linegraphs

<!--chapter:end:93-appendixC.Rmd-->


# Learning Check Solutions {#appendixD}

Placeholder


## Chapter 2 Solutions 
## Chapter 3 Solutions 
## Chapter 4 Solutions
## Chapter 5 Solutions
## Chapter 6 Solutions

<!--chapter:end:94-appendixD.Rmd-->


# Archive HR datasets {#appendixE}

Placeholder


## Gender Pay Gap {#gender_pay_gap}
## Overhead value analysis {#overhead}
## HR Service Desk {#service_desk_data}
## HR recruitment, selection and performance data {#HRrecruitment}
## Job classification
## Job classification
## Absenteeism at work
## Job classification

<!--chapter:end:95-appendixE.Rmd-->

`r if(knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:99-references.Rmd-->

